{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMUju8SwjpasCs6+rE+H5no",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/issei4683/issei4683-SMBC-Group-GREEN-DATA-Challenge-2024/blob/main/SMBC_Group_GREEN%C3%97DATA_Challenge_2024_code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ENe_r75sLyk1"
      },
      "outputs": [],
      "source": [
        "# Google Driveのマウント\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 必要なライブラリのインストール\n",
        "!pip install pygeohash\n",
        "!pip install catboost\n",
        "\n",
        "# 警告を無効化（不要なメッセージを抑制）\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# 基本ライブラリ\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# 進捗表示や効率化\n",
        "from tqdm import tqdm\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "import gc  # メモリ管理\n",
        "\n",
        "# モデリングライブラリ\n",
        "from lightgbm import LGBMRegressor\n",
        "from catboost import CatBoostRegressor\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.ensemble import HistGradientBoostingRegressor\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# 補助ライブラリ（モデル評価、データ前処理）\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import *\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# 特殊ライブラリ\n",
        "import pygeohash as pgh  # 緯度経度データのエンコード\n",
        "import joblib  # モデル保存・読み込み\n",
        "import math  # 数学関数\n",
        "\n",
        "# システム処理\n",
        "import os\n"
      ],
      "metadata": {
        "id": "4_xpQMO7L7do",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# データの読み込み\n",
        "train_df = pd.read_csv('/content/drive/MyDrive/data/train (1).csv')\n",
        "test_df = pd.read_csv('/content/drive/MyDrive/data/test (1).csv')\n",
        "del train_df['Unnamed: 0'], test_df['Unnamed: 0']\n",
        "del train_df['FacilityName'], test_df['FacilityName']\n",
        "del train_df['LocationAddress'], test_df['LocationAddress']\n",
        "del train_df['ZIP'], test_df['ZIP']\n",
        "del train_df['IndustryType'], test_df['IndustryType']\n",
        "del train_df['SecondPrimaryNAICS'], test_df['SecondPrimaryNAICS']"
      ],
      "metadata": {
        "id": "gTga0QLiL9gO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#探索データ分析（EDA）\n",
        "\n",
        "###🎯目的\n",
        "\n",
        "-📜データセットの概要を確認する。\n",
        "\n",
        "-📊分布、関係、パターンを視覚化する。\n",
        "\n",
        "-🕵️‍♀️データに関する洞察を得る。\n",
        "\n",
        "-🧹欠損値、外れ値、データの不整合を特定します。"
      ],
      "metadata": {
        "id": "9PUO5cN4nsuF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##📜データセットの概要"
      ],
      "metadata": {
        "id": "qj15H2qhoJP7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#データセットの形状と最初の行を確認\n",
        "train_df.head()"
      ],
      "metadata": {
        "id": "G6eVoVU6iDYB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#カラム名、入力済行数、タイプの確認\n",
        "train_df.info()"
      ],
      "metadata": {
        "id": "8qvMLzK0iUSx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#数値列の基本統計を確認\n",
        "train_df.describe().round(2)"
      ],
      "metadata": {
        "id": "WLWhBDjyAnoS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# グラフサイズの設定\n",
        "plt.figure(figsize=(15, 10))\n",
        "\n",
        "# 欠損値の位置をヒートマップで可視化\n",
        "sns.heatmap(train_df.isnull(), cmap='viridis', cbar=False, yticklabels=False)\n",
        "\n",
        "# グラフタイトルとラベル\n",
        "plt.title('Missing Values Heatmap', fontsize=18)\n",
        "plt.xlabel('Columns', fontsize=14)\n",
        "plt.ylabel('Records', fontsize=14)\n",
        "\n",
        "# 表示\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "cuLeCh0JFMji"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 数値列に限定したデータフレームを取得\n",
        "numeric_data = train_df.select_dtypes(include=['float64', 'int64'])\n",
        "\n",
        "# 相関行列を計算\n",
        "correlation_matrix = numeric_data.corr()\n",
        "\n",
        "# 相関行列の可視化\n",
        "plt.figure(figsize=(12, 8))\n",
        "sns.heatmap(correlation_matrix, annot=True, fmt=\".2f\", cmap=\"coolwarm\", cbar=True)\n",
        "plt.title(\"Correlation Matrix\", fontsize=16)\n",
        "plt.xticks(rotation=45)\n",
        "plt.yticks(rotation=0)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "OQJXTwrMHAH9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 🛠️ 特徴量エンジニアリング"
      ],
      "metadata": {
        "id": "h9d5lnk3tl-4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Primary NAICSの前処理をするための準備をする。\n",
        "# このマッピングは https://www.census.gov/naics/ から取得され、2007 年の NAICS マッピングに従う。\n",
        "\n",
        "two_digit_map     = {11: 'Agriculture, Forestry, Fishing and Hunting',\n",
        "                    21: 'Mining, Quarrying, and Oil and Gas Extraction',\n",
        "                    22: 'Utilities',\n",
        "                    23: 'Construction',\n",
        "                    31: 'Manufacturing',\n",
        "                    32: 'Manufacturing',\n",
        "                    33: 'Manufacturing',\n",
        "                    42: 'Wholesale Trade',\n",
        "                    44: 'Retail Trade',\n",
        "                    45: 'Retail Trade',\n",
        "                    48: 'Transportation and Warehousing',\n",
        "                    49: 'Transportation and Warehousing',\n",
        "                    51: 'Information',\n",
        "                    52: 'Finance and Insurance',\n",
        "                    53: 'Real Estate and Rental and Leasing',\n",
        "                    54: 'Professional, Scientific, and Technical Services',\n",
        "                    55: 'Management of Companies and Enterprises',\n",
        "                    56: 'Administrative and Support and Waste Management and Remediation Services',\n",
        "                    61: 'Educational Services',\n",
        "                    62: 'Health Care and Social Assistance',\n",
        "                    71: 'Arts, Entertainment, and Recreation',\n",
        "                    72: 'Accommodation and Food Services',\n",
        "                    81: 'Other Services (except Public Administration)',\n",
        "                    92: 'Public Administration'}"
      ],
      "metadata": {
        "id": "lCCEqWHaL-Nu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Economic_Sector列を作成し、Numpy配列形式に変換して、後続の処理やモデル学習での操作性を向上させる。\n",
        "\n",
        "\n",
        "# PrimaryNAICS列から最初の2桁を抽出して整数型に変換\n",
        "train_df['first_two_digit_primary_naics'] = train_df['PrimaryNAICS'].apply(lambda z: str(z)[:2]).astype(int)\n",
        "test_df['first_two_digit_primary_naics']  = test_df['PrimaryNAICS'].apply(lambda z: str(z)[:2]).astype(int)\n",
        "\n",
        "# two_digit_mapを使用して経済セクターを割り当て\n",
        "train_df['Economic_Sector']               = train_df['first_two_digit_primary_naics'].map(two_digit_map)\n",
        "test_df['Economic_Sector']                = test_df['first_two_digit_primary_naics'].map(two_digit_map)\n",
        "\n",
        "del train_df['first_two_digit_primary_naics'], test_df['first_two_digit_primary_naics'] #一時的に使用した列を削除\n",
        "\n",
        "# Economic_Sector列の値を配列に変換\n",
        "econ_sector_train                         = train_df['Economic_Sector'].values\n",
        "econ_sector_test                          = test_df['Economic_Sector'].values"
      ],
      "metadata": {
        "id": "dKTENguPMK_3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 経済セクターに関して最も近い 5 つの隣接距離を計算します\n",
        "\n",
        "def haversine(lat1, lon1, lat2, lon2):\n",
        "    # 緯度と経度を度からラジアンに変換する\n",
        "    lat1, lon1, lat2, lon2 = map(math.radians, [lat1, lon1, lat2, lon2])\n",
        "\n",
        "    # ハーバーサイン式（地球上の2点間の距離を求める）\n",
        "    dlat = lat2 - lat1\n",
        "    dlon = lon2 - lon1\n",
        "    a = math.sin(dlat / 2)**2 + math.cos(lat1) * math.cos(lat2) * math.sin(dlon / 2)**2\n",
        "    c = 2 * math.atan2(math.sqrt(a), math.sqrt(1 - a))\n",
        "\n",
        "    # 地球の半径（キロメートル）（平均半径）\n",
        "    R = 6371.0\n",
        "\n",
        "    # 距離を計算する\n",
        "    distance = R * c\n",
        "    return distance"
      ],
      "metadata": {
        "id": "WacyQJCTMN8r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 周辺観測地点の情報を考慮した特量量を作成\n",
        "\n",
        "# neighboursの設定\n",
        "neighbours = 5\n",
        "\n",
        "# 保存するファイルパスの設定\n",
        "train_distance_file = f\"/content/drive/MyDrive/data/train_distance_{neighbours}nbrs.csv\"\n",
        "test_distance_file = f\"/content/drive/MyDrive/data/test_distance_{neighbours}nbrs.csv\"\n",
        "\n",
        "# test_distanceの読み込みまたは計算\n",
        "if os.path.exists(test_distance_file):\n",
        "    print(f\"{test_distance_file} exists. Loading from file.\")\n",
        "    test_distance = pd.read_csv(test_distance_file)\n",
        "else:\n",
        "    output = []\n",
        "    for index in tqdm(range(test_df.shape[0])):\n",
        "        lat1 = test_df.iloc[index]['Latitude']\n",
        "        lon1 = test_df.iloc[index]['Longitude']\n",
        "        econ_sector_ref = test_df.iloc[index]['Economic_Sector']\n",
        "        x = get_nearest_distance(lat1, lon1, econ_sector_ref, neighbours=neighbours, train_point=False)\n",
        "        output.append(x)\n",
        "\n",
        "    test_distance = pd.DataFrame(output, columns=['Economy_Sector_Weighted_Avg', 'Economic_Sector_Average',\n",
        "                                                  'Nearest_Weighted_Average', 'Nearest_Average'])\n",
        "    test_distance.to_csv(test_distance_file, index=False)\n",
        "\n",
        "# train_distanceの読み込みまたは計算\n",
        "if os.path.exists(train_distance_file):\n",
        "    print(f\"{train_distance_file} exists. Loading from file.\")\n",
        "    train_distance = pd.read_csv(train_distance_file)\n",
        "else:\n",
        "    output = []\n",
        "    for index in tqdm(range(train_df.shape[0])):\n",
        "        lat1 = train_df.iloc[index]['Latitude']\n",
        "        lon1 = train_df.iloc[index]['Longitude']\n",
        "        econ_sector_ref = train_df.iloc[index]['Economic_Sector']\n",
        "        x = get_nearest_distance(lat1, lon1, econ_sector_ref, neighbours=neighbours, train_point=True)\n",
        "        output.append(x)\n",
        "\n",
        "    train_distance = pd.DataFrame(output, columns=['Economy_Sector_Weighted_Avg', 'Economic_Sector_Average',\n",
        "                                                   'Nearest_Weighted_Average', 'Nearest_Average'])\n",
        "    train_distance.to_csv(train_distance_file, index=False)\n",
        "\n",
        "# データフレームをtrain_dfとtest_dfに結合\n",
        "train_df = pd.concat((train_df, train_distance), axis=1)\n",
        "test_df  = pd.concat((test_df, test_distance), axis=1)\n"
      ],
      "metadata": {
        "id": "ObJ4ZnU_fH5l",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_distance(lat1, lon1, lat2, lon2):\n",
        "    return haversine(lat1, lon1, lat2, lon2)\n",
        "\n",
        "def get_nearest_distance(lat1, lon1, econ_sector_ref, neighbours=5, train_point=False):\n",
        "    # 必要な列をNumPy配列に一度抽出する\n",
        "    latitudes = train_df['Latitude'].values\n",
        "    longitudes = train_df['Longitude'].values\n",
        "    ghg_emissions = train_df['GHG_Direct_Emissions_14_in_metric_tons'].values\n",
        "    econ_sectors = train_df['Economic_Sector'].values\n",
        "\n",
        "    # NaN排出を直接フィルタリングする\n",
        "    valid_indices = ~np.isnan(ghg_emissions)\n",
        "\n",
        "    latitudes = latitudes[valid_indices]\n",
        "    longitudes = longitudes[valid_indices]\n",
        "    ghg_emissions = ghg_emissions[valid_indices]\n",
        "    econ_sectors = econ_sectors[valid_indices]\n",
        "\n",
        "    # ThreadPoolExecutorを使用して距離計算を並列化する\n",
        "    with ThreadPoolExecutor() as executor:\n",
        "        distances = list(executor.map(calculate_distance,\n",
        "                                     [lat1]*len(latitudes),\n",
        "                                     [lon1]*len(longitudes),\n",
        "                                     latitudes,\n",
        "                                     longitudes))\n",
        "\n",
        "    # 結果をDataFrameに結合する\n",
        "    near_df = pd.DataFrame({\n",
        "        'Distance': distances,\n",
        "        'GHG_emission_14': ghg_emissions,\n",
        "        'Economic_Sector': econ_sectors\n",
        "    })\n",
        "\n",
        "\n",
        "    if train_point:\n",
        "        near_df.sort_values(by='Distance', inplace=True)\n",
        "        near_df = near_df.dropna()\n",
        "        near_df = near_df.iloc[1:].reset_index(drop=True)\n",
        "    else:\n",
        "        near_df.sort_values(by='Distance', inplace=True)\n",
        "        near_df = near_df.dropna()\n",
        "\n",
        "    # 経済分野別に絞り込む\n",
        "    nearest_locations_econ_sector = near_df[near_df['Economic_Sector'] == econ_sector_ref]\n",
        "\n",
        "    # 経済分野と全体の両方で最も近い上位Nを取得します\n",
        "    sub_near_econ = nearest_locations_econ_sector.head(neighbours)\n",
        "    sub_nearest_locations = near_df.head(neighbours)\n",
        "\n",
        "    # 加重平均を計算する\n",
        "    econ_weighted_average = (sub_near_econ['GHG_emission_14'] / np.where(sub_near_econ['Distance'] == 0, 1, sub_near_econ['Distance']) ).sum()\n",
        "    near_weighted_average = (sub_nearest_locations['GHG_emission_14'] / np.where(sub_nearest_locations['Distance']==0,1,sub_nearest_locations['Distance'])).sum()\n",
        "\n",
        "    # 通常の平均値を計算する\n",
        "    econ_average = sub_near_econ['GHG_emission_14'].mean()\n",
        "    near_average = sub_nearest_locations['GHG_emission_14'].mean()\n",
        "\n",
        "    return [econ_weighted_average, econ_average, near_weighted_average, near_average]"
      ],
      "metadata": {
        "id": "JBg1PUTFMSTq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TRI排出量の特徴量作成\n",
        "\n",
        "def create_features1(df):\n",
        "\n",
        "    # 前年比\n",
        "    df['TRI_Air_Emissions_YoY_Change_11'] = df['TRI_Air_Emissions_11_in_lbs'] - df['TRI_Air_Emissions_10_in_lbs']\n",
        "    df['TRI_Air_Emissions_YoY_Change_12'] = df['TRI_Air_Emissions_12_in_lbs'] - df['TRI_Air_Emissions_11_in_lbs']\n",
        "    df['TRI_Air_Emissions_YoY_Change_13'] = df['TRI_Air_Emissions_13_in_lbs'] - df['TRI_Air_Emissions_12_in_lbs']\n",
        "\n",
        "   # 前年比増加率\n",
        "    df['TRI_Air_Emissions_Growth_Rate_11'] = np.where(\n",
        "        df['TRI_Air_Emissions_10_in_lbs'].notna() & (df['TRI_Air_Emissions_10_in_lbs'] != 0),\n",
        "        (df['TRI_Air_Emissions_11_in_lbs'] - df['TRI_Air_Emissions_10_in_lbs']) / df['TRI_Air_Emissions_10_in_lbs'],\n",
        "        np.nan\n",
        "    )\n",
        "\n",
        "    df['TRI_Air_Emissions_Growth_Rate_12'] = np.where(\n",
        "        df['TRI_Air_Emissions_11_in_lbs'].notna() & (df['TRI_Air_Emissions_11_in_lbs'] != 0),\n",
        "        (df['TRI_Air_Emissions_12_in_lbs'] - df['TRI_Air_Emissions_11_in_lbs']) / df['TRI_Air_Emissions_11_in_lbs'],\n",
        "        np.nan\n",
        "    )\n",
        "\n",
        "    df['TRI_Air_Emissions_Growth_Rate_13'] = np.where(\n",
        "        df['TRI_Air_Emissions_12_in_lbs'].notna() & (df['TRI_Air_Emissions_12_in_lbs'] != 0),\n",
        "        (df['TRI_Air_Emissions_13_in_lbs'] - df['TRI_Air_Emissions_12_in_lbs']) / df['TRI_Air_Emissions_12_in_lbs'],\n",
        "        np.nan\n",
        "    )\n",
        "\n",
        "    return df\n",
        "\n",
        "train_df      = create_features1(train_df)\n",
        "test_df       = create_features1(test_df)\n",
        "new_features1 = ['TRI_Air_Emissions_YoY_Change_11','TRI_Air_Emissions_YoY_Change_12','TRI_Air_Emissions_YoY_Change_13',\n",
        "                 'TRI_Air_Emissions_Growth_Rate_11','TRI_Air_Emissions_Growth_Rate_12','TRI_Air_Emissions_Growth_Rate_13']"
      ],
      "metadata": {
        "id": "p_Udt9Z6kwrl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# GHG排出量の特徴量作成\n",
        "\n",
        "def create_features2(df):\n",
        "\n",
        "    # 前年比\n",
        "    df['GHG_Direct_Emissions_YoY_Change_11'] = df['GHG_Direct_Emissions_11_in_metric_tons'] - df['GHG_Direct_Emissions_10_in_metric_tons']\n",
        "    df['GHG_Direct_Emissions_YoY_Change_12'] = df['GHG_Direct_Emissions_12_in_metric_tons'] - df['GHG_Direct_Emissions_11_in_metric_tons']\n",
        "    df['GHG_Direct_Emissions_YoY_Change_13'] = df['GHG_Direct_Emissions_13_in_metric_tons'] - df['GHG_Direct_Emissions_12_in_metric_tons']\n",
        "\n",
        "    # 前年比増加率\n",
        "    df['GHG_Direct_Emissions_Growth_Rate_11'] = np.where(\n",
        "        df['GHG_Direct_Emissions_10_in_metric_tons'].notna() & (df['GHG_Direct_Emissions_10_in_metric_tons'] != 0),\n",
        "        (df['GHG_Direct_Emissions_11_in_metric_tons'] - df['GHG_Direct_Emissions_10_in_metric_tons']) / df['GHG_Direct_Emissions_10_in_metric_tons'],\n",
        "        np.nan\n",
        "    )\n",
        "\n",
        "    df['GHG_Direct_Emissions_Growth_Rate_12'] = np.where(\n",
        "        df['GHG_Direct_Emissions_11_in_metric_tons'].notna() & (df['GHG_Direct_Emissions_11_in_metric_tons'] != 0),\n",
        "        (df['GHG_Direct_Emissions_12_in_metric_tons'] - df['GHG_Direct_Emissions_11_in_metric_tons']) / df['GHG_Direct_Emissions_11_in_metric_tons'],\n",
        "        np.nan\n",
        "    )\n",
        "\n",
        "    df['GHG_Direct_Emissions_Growth_Rate_13'] = np.where(\n",
        "        df['GHG_Direct_Emissions_12_in_metric_tons'].notna() & (df['GHG_Direct_Emissions_12_in_metric_tons'] != 0),\n",
        "        (df['GHG_Direct_Emissions_13_in_metric_tons'] - df['GHG_Direct_Emissions_12_in_metric_tons']) / df['GHG_Direct_Emissions_12_in_metric_tons'],\n",
        "        np.nan\n",
        "    )\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "new_features2 = ['GHG_Direct_Emissions_YoY_Change_11','GHG_Direct_Emissions_YoY_Change_12','GHG_Direct_Emissions_YoY_Change_13',\n",
        "                 'GHG_Direct_Emissions_Growth_Rate_11','GHG_Direct_Emissions_Growth_Rate_12','GHG_Direct_Emissions_Growth_Rate_13'\n",
        "                 ]\n",
        "train_df      = create_features2(train_df)\n",
        "test_df       = create_features2(test_df)"
      ],
      "metadata": {
        "id": "sf0X49TO83PO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TRIとGHGの比率の特徴量作成\n",
        "\n",
        "def create_features3(df):\n",
        "\n",
        "    # 比率計算時に0除算を防ぐための小さな値を追加\n",
        "    epsilon = 1e-6\n",
        "\n",
        "    # 各年のTRI排出量とGHG排出量の比率を計算\n",
        "    df['TRI_to_GHG_Ratio_10'] = df['TRI_Air_Emissions_10_in_lbs'] / (df['GHG_Direct_Emissions_10_in_metric_tons'] + epsilon)\n",
        "    df['TRI_to_GHG_Ratio_11'] = df['TRI_Air_Emissions_11_in_lbs'] / (df['GHG_Direct_Emissions_11_in_metric_tons'] + epsilon)\n",
        "    df['TRI_to_GHG_Ratio_12'] = df['TRI_Air_Emissions_12_in_lbs'] / (df['GHG_Direct_Emissions_12_in_metric_tons'] + epsilon)\n",
        "    df['TRI_to_GHG_Ratio_13'] = df['TRI_Air_Emissions_13_in_lbs'] / (df['GHG_Direct_Emissions_13_in_metric_tons'] + epsilon)\n",
        "\n",
        "    return df\n",
        "\n",
        "train_df      = create_features3(train_df)\n",
        "test_df       = create_features3(test_df)\n",
        "new_features3 = ['TRI_to_GHG_Ratio_10','TRI_to_GHG_Ratio_11','TRI_to_GHG_Ratio_12','TRI_to_GHG_Ratio_13']"
      ],
      "metadata": {
        "id": "o6Kg94US86tZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 🪄 データ前処理"
      ],
      "metadata": {
        "id": "ChntQDciuTI-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 経済セクターと州に対して行われた目標の集計\n",
        "\n",
        "# summary_df の作成\n",
        "summary_df = train_df.groupby(['Economic_Sector', 'State']).agg({\n",
        "    'GHG_Direct_Emissions_14_in_metric_tons': ['mean', 'median', 'max', 'min', 'count']\n",
        "})\n",
        "\n",
        "# 列名をリネーム\n",
        "summary_df.columns = [\n",
        "    'GHG_Direct_Emissions_14_in_metric_tons_mean',\n",
        "    'GHG_Direct_Emissions_14_in_metric_tons_median',\n",
        "    'GHG_Direct_Emissions_14_in_metric_tons_max',\n",
        "    'GHG_Direct_Emissions_14_in_metric_tons_min',\n",
        "    'GHG_Direct_Emissions_14_in_metric_tons_count'\n",
        "]\n",
        "\n",
        "# インデックスをリセットして、まとめたデータフレームにする\n",
        "summary_df = summary_df.reset_index()\n",
        "\n",
        "# train_df と test_df に対して、 summary_df を left join\n",
        "train_df = train_df.merge(summary_df, on=['Economic_Sector', 'State'], how='left')\n",
        "test_df = test_df.merge(summary_df, on=['Economic_Sector', 'State'], how='left')\n"
      ],
      "metadata": {
        "id": "8gLIUUIOjRyD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# カテゴリ特徴量のエンコード\n",
        "\n",
        "merged_df = pd.concat((train_df,test_df),axis=0)\n",
        "for cols in ['City','State','County','FIPScode','PrimaryNAICS','Economic_Sector']:\n",
        "    le              = LabelEncoder()\n",
        "    merged_df[cols] = le.fit_transform(merged_df[cols].values.reshape(-1,1))\n",
        "\n",
        "train_df = merged_df.iloc[:train_df.shape[0],:]\n",
        "test_df  = merged_df.iloc[train_df.shape[0]:,:]\n",
        "train_df.shape,test_df.shape"
      ],
      "metadata": {
        "id": "XKnn5sTTMv-_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 🏋️ モデルトレーニング"
      ],
      "metadata": {
        "id": "v-1Pobi-qkUa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# モデルトレーニング用の最終データの作成\n",
        "\n",
        "numerical_columns = [\n",
        "                    'TRI_Air_Emissions_10_in_lbs', 'TRI_Air_Emissions_11_in_lbs',\n",
        "                    'TRI_Air_Emissions_12_in_lbs', 'TRI_Air_Emissions_13_in_lbs',\n",
        "                    'GHG_Direct_Emissions_10_in_metric_tons', 'GHG_Direct_Emissions_11_in_metric_tons',\n",
        "                    'GHG_Direct_Emissions_12_in_metric_tons', 'GHG_Direct_Emissions_13_in_metric_tons',\n",
        "                    ]\n",
        "\n",
        "categorical_columns = ['City','State','County','FIPScode','PrimaryNAICS','Economic_Sector']\n",
        "\n",
        "lat_lon_columns = ['Latitude','Longitude']\n",
        "\n",
        "new_features1  = [\n",
        "                  'TRI_Air_Emissions_YoY_Change_11','TRI_Air_Emissions_YoY_Change_12','TRI_Air_Emissions_YoY_Change_13',\n",
        "                  'TRI_Air_Emissions_Growth_Rate_11','TRI_Air_Emissions_Growth_Rate_12','TRI_Air_Emissions_Growth_Rate_13'\n",
        "                 ]\n",
        "\n",
        "new_features2  = [\n",
        "                  'GHG_Direct_Emissions_YoY_Change_11','GHG_Direct_Emissions_YoY_Change_12','GHG_Direct_Emissions_YoY_Change_13',\n",
        "                  'GHG_Direct_Emissions_Growth_Rate_11','GHG_Direct_Emissions_Growth_Rate_12','GHG_Direct_Emissions_Growth_Rate_13'\n",
        "                 ]\n",
        "\n",
        "new_features3 = ['TRI_to_GHG_Ratio_10','TRI_to_GHG_Ratio_11','TRI_to_GHG_Ratio_12','TRI_to_GHG_Ratio_13']\n",
        "\n",
        "target_columns = ['GHG_Direct_Emissions_14_in_metric_tons']\n",
        "\n",
        "train = train_df[numerical_columns+\n",
        "                 lat_lon_columns+\n",
        "                # categorical_columns+\n",
        "                 new_features1+['PrimaryNAICS']\n",
        "                #  new_features2+\n",
        "                #  new_features3+\n",
        "                #  train_aggregations+\n",
        "                #  neighbour_feats\n",
        "                 ].values\n",
        "test  = test_df[numerical_columns+\n",
        "                 lat_lon_columns+\n",
        "                # categorical_columns+\n",
        "                 new_features1+['PrimaryNAICS']\n",
        "                #  new_features2+\n",
        "                #  new_features3+\n",
        "                #  train_aggregations+\n",
        "                #  neighbour_feats\n",
        "                 ].values\n",
        "target = train_df[target_columns].values"
      ],
      "metadata": {
        "id": "TIuR260RM2tr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 📝 モデル評価"
      ],
      "metadata": {
        "id": "1xEw26bPvRrF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.ensemble import HistGradientBoostingRegressor\n",
        "from catboost import CatBoostRegressor\n",
        "\n",
        "# 各foldでの予測結果を格納する配列の初期化\n",
        "stacked_train = np.zeros((train.shape[0], 3))  # model_dictに3つのモデルがある場合\n",
        "stacked_test = np.zeros((test.shape[0], 3))\n",
        "\n",
        "# アンサンブルの重み\n",
        "weights = [0.2, 0.5, 0.3]  # Random Forest : CatBoost : LightGBM\n",
        "\n",
        "def get_models_trained(train, test, target, num_folds):\n",
        "    kf = KFold(n_splits=num_folds, shuffle=True, random_state=13)\n",
        "\n",
        "    oof_predictions = np.zeros(len(train))\n",
        "    test_predictions = np.zeros(len(test))\n",
        "\n",
        "    for fold, (train_index, valid_index) in enumerate(kf.split(train, target)):\n",
        "        X_train, X_valid = train[train_index], train[valid_index]\n",
        "        y_train, y_valid = target[train_index], target[valid_index]\n",
        "\n",
        "        # モデルを定義\n",
        "        # Random Forestモデル\n",
        "        rf_params = {\n",
        "            \"n_estimators\": 100,\n",
        "            \"max_depth\": 10,\n",
        "            \"random_state\": 13\n",
        "        }\n",
        "        model1 = RandomForestRegressor(**rf_params)\n",
        "\n",
        "        # CatBoostモデル\n",
        "        catboost_params = {\n",
        "            \"iterations\": 300,\n",
        "            \"learning_rate\": 0.03,\n",
        "            \"depth\": 6,\n",
        "            \"random_seed\": 13,\n",
        "            \"silent\": True\n",
        "        }\n",
        "        model2 = CatBoostRegressor(**catboost_params)\n",
        "\n",
        "        # LightGBMモデル\n",
        "        lgbm_params = {\n",
        "            \"n_estimators\": 100,\n",
        "            \"learning_rate\": 0.05,\n",
        "            \"max_depth\": 6,\n",
        "            \"random_state\": 13\n",
        "        }\n",
        "        model3 = LGBMRegressor(**lgbm_params)\n",
        "\n",
        "        # モデルリストに追加\n",
        "        models = [model1, model2, model3]  # Random Forest, CatBoost, LightGBM\n",
        "\n",
        "        for i, model in enumerate(models):\n",
        "            # モデルの訓練と予測\n",
        "            _ = model.fit(X_train, np.log1p(y_train))\n",
        "            valid_preds = np.expm1(model.predict(X_valid))\n",
        "            test_preds = np.expm1(model.predict(test)) / kf.n_splits\n",
        "\n",
        "            # RMSLEスコアの計算\n",
        "            rmsle_score = root_mean_squared_log_error(y_valid, valid_preds)\n",
        "            print(f\"Fold {fold + 1} RMSLE for model {i + 1} = {rmsle_score}\")\n",
        "\n",
        "            # スタッキング用に予測結果を格納\n",
        "            stacked_train[valid_index, i] = valid_preds\n",
        "            stacked_test[:, i] += test_preds\n",
        "\n",
        "    # 平均予測を重み付きで算出\n",
        "    final_oof_predictions = np.sum(stacked_train * np.array(weights), axis=1)\n",
        "    final_test_predictions = np.sum(stacked_test * np.array(weights), axis=1)\n",
        "\n",
        "    # RMSLEスコアの表示\n",
        "    oof_rmsle = root_mean_squared_log_error(target, final_oof_predictions)\n",
        "    print(f\"OOF RMSLE = {oof_rmsle}\")\n",
        "\n",
        "    return final_oof_predictions, final_test_predictions\n"
      ],
      "metadata": {
        "id": "qEawQCbC8xk9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 実行\n",
        "oof_predictions, test_predictions = get_models_trained(train, test, target, num_folds=30)"
      ],
      "metadata": {
        "id": "1rMm2dvSNcf7",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 提出ファイルの作成\n",
        "submit = pd.read_csv('/content/drive/MyDrive/data/sample_submission.csv', header=None)\n",
        "submit [ 1 ]  =  test_predictions\n",
        "submit . to_csv ( 'submission_36_g.csv' ,  header = None ,  index = False )"
      ],
      "metadata": {
        "id": "lqS77bJCNE4j"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}