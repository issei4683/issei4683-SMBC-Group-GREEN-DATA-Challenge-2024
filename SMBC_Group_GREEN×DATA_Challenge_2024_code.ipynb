{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMUju8SwjpasCs6+rE+H5no",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/issei4683/issei4683-SMBC-Group-GREEN-DATA-Challenge-2024/blob/main/SMBC_Group_GREEN%C3%97DATA_Challenge_2024_code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ENe_r75sLyk1"
      },
      "outputs": [],
      "source": [
        "# Google Driveã®ãƒã‚¦ãƒ³ãƒˆ\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\n",
        "!pip install pygeohash\n",
        "!pip install catboost\n",
        "\n",
        "# è­¦å‘Šã‚’ç„¡åŠ¹åŒ–ï¼ˆä¸è¦ãªãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’æŠ‘åˆ¶ï¼‰\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# åŸºæœ¬ãƒ©ã‚¤ãƒ–ãƒ©ãƒª\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# é€²æ—è¡¨ç¤ºã‚„åŠ¹ç‡åŒ–\n",
        "from tqdm import tqdm\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "import gc  # ãƒ¡ãƒ¢ãƒªç®¡ç†\n",
        "\n",
        "# ãƒ¢ãƒ‡ãƒªãƒ³ã‚°ãƒ©ã‚¤ãƒ–ãƒ©ãƒª\n",
        "from lightgbm import LGBMRegressor\n",
        "from catboost import CatBoostRegressor\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.ensemble import HistGradientBoostingRegressor\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# è£œåŠ©ãƒ©ã‚¤ãƒ–ãƒ©ãƒªï¼ˆãƒ¢ãƒ‡ãƒ«è©•ä¾¡ã€ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†ï¼‰\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import *\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# ç‰¹æ®Šãƒ©ã‚¤ãƒ–ãƒ©ãƒª\n",
        "import pygeohash as pgh  # ç·¯åº¦çµŒåº¦ãƒ‡ãƒ¼ã‚¿ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰\n",
        "import joblib  # ãƒ¢ãƒ‡ãƒ«ä¿å­˜ãƒ»èª­ã¿è¾¼ã¿\n",
        "import math  # æ•°å­¦é–¢æ•°\n",
        "\n",
        "# ã‚·ã‚¹ãƒ†ãƒ å‡¦ç†\n",
        "import os\n"
      ],
      "metadata": {
        "id": "4_xpQMO7L7do",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿\n",
        "train_df = pd.read_csv('/content/drive/MyDrive/data/train (1).csv')\n",
        "test_df = pd.read_csv('/content/drive/MyDrive/data/test (1).csv')\n",
        "del train_df['Unnamed: 0'], test_df['Unnamed: 0']\n",
        "del train_df['FacilityName'], test_df['FacilityName']\n",
        "del train_df['LocationAddress'], test_df['LocationAddress']\n",
        "del train_df['ZIP'], test_df['ZIP']\n",
        "del train_df['IndustryType'], test_df['IndustryType']\n",
        "del train_df['SecondPrimaryNAICS'], test_df['SecondPrimaryNAICS']"
      ],
      "metadata": {
        "id": "gTga0QLiL9gO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#æ¢ç´¢ãƒ‡ãƒ¼ã‚¿åˆ†æï¼ˆEDAï¼‰\n",
        "\n",
        "###ğŸ¯ç›®çš„\n",
        "\n",
        "-ğŸ“œãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®æ¦‚è¦ã‚’ç¢ºèªã™ã‚‹ã€‚\n",
        "\n",
        "-ğŸ“Šåˆ†å¸ƒã€é–¢ä¿‚ã€ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’è¦–è¦šåŒ–ã™ã‚‹ã€‚\n",
        "\n",
        "-ğŸ•µï¸â€â™€ï¸ãƒ‡ãƒ¼ã‚¿ã«é–¢ã™ã‚‹æ´å¯Ÿã‚’å¾—ã‚‹ã€‚\n",
        "\n",
        "-ğŸ§¹æ¬ æå€¤ã€å¤–ã‚Œå€¤ã€ãƒ‡ãƒ¼ã‚¿ã®ä¸æ•´åˆã‚’ç‰¹å®šã—ã¾ã™ã€‚"
      ],
      "metadata": {
        "id": "9PUO5cN4nsuF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##ğŸ“œãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®æ¦‚è¦"
      ],
      "metadata": {
        "id": "qj15H2qhoJP7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®å½¢çŠ¶ã¨æœ€åˆã®è¡Œã‚’ç¢ºèª\n",
        "train_df.head()"
      ],
      "metadata": {
        "id": "G6eVoVU6iDYB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#ã‚«ãƒ©ãƒ åã€å…¥åŠ›æ¸ˆè¡Œæ•°ã€ã‚¿ã‚¤ãƒ—ã®ç¢ºèª\n",
        "train_df.info()"
      ],
      "metadata": {
        "id": "8qvMLzK0iUSx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#æ•°å€¤åˆ—ã®åŸºæœ¬çµ±è¨ˆã‚’ç¢ºèª\n",
        "train_df.describe().round(2)"
      ],
      "metadata": {
        "id": "WLWhBDjyAnoS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ã‚°ãƒ©ãƒ•ã‚µã‚¤ã‚ºã®è¨­å®š\n",
        "plt.figure(figsize=(15, 10))\n",
        "\n",
        "# æ¬ æå€¤ã®ä½ç½®ã‚’ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ã§å¯è¦–åŒ–\n",
        "sns.heatmap(train_df.isnull(), cmap='viridis', cbar=False, yticklabels=False)\n",
        "\n",
        "# ã‚°ãƒ©ãƒ•ã‚¿ã‚¤ãƒˆãƒ«ã¨ãƒ©ãƒ™ãƒ«\n",
        "plt.title('Missing Values Heatmap', fontsize=18)\n",
        "plt.xlabel('Columns', fontsize=14)\n",
        "plt.ylabel('Records', fontsize=14)\n",
        "\n",
        "# è¡¨ç¤º\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "cuLeCh0JFMji"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# æ•°å€¤åˆ—ã«é™å®šã—ãŸãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’å–å¾—\n",
        "numeric_data = train_df.select_dtypes(include=['float64', 'int64'])\n",
        "\n",
        "# ç›¸é–¢è¡Œåˆ—ã‚’è¨ˆç®—\n",
        "correlation_matrix = numeric_data.corr()\n",
        "\n",
        "# ç›¸é–¢è¡Œåˆ—ã®å¯è¦–åŒ–\n",
        "plt.figure(figsize=(12, 8))\n",
        "sns.heatmap(correlation_matrix, annot=True, fmt=\".2f\", cmap=\"coolwarm\", cbar=True)\n",
        "plt.title(\"Correlation Matrix\", fontsize=16)\n",
        "plt.xticks(rotation=45)\n",
        "plt.yticks(rotation=0)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "OQJXTwrMHAH9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ğŸ› ï¸ ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°"
      ],
      "metadata": {
        "id": "h9d5lnk3tl-4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Primary NAICSã®å‰å‡¦ç†ã‚’ã™ã‚‹ãŸã‚ã®æº–å‚™ã‚’ã™ã‚‹ã€‚\n",
        "# ã“ã®ãƒãƒƒãƒ”ãƒ³ã‚°ã¯ https://www.census.gov/naics/ ã‹ã‚‰å–å¾—ã•ã‚Œã€2007 å¹´ã® NAICS ãƒãƒƒãƒ”ãƒ³ã‚°ã«å¾“ã†ã€‚\n",
        "\n",
        "two_digit_map     = {11: 'Agriculture, Forestry, Fishing and Hunting',\n",
        "                    21: 'Mining, Quarrying, and Oil and Gas Extraction',\n",
        "                    22: 'Utilities',\n",
        "                    23: 'Construction',\n",
        "                    31: 'Manufacturing',\n",
        "                    32: 'Manufacturing',\n",
        "                    33: 'Manufacturing',\n",
        "                    42: 'Wholesale Trade',\n",
        "                    44: 'Retail Trade',\n",
        "                    45: 'Retail Trade',\n",
        "                    48: 'Transportation and Warehousing',\n",
        "                    49: 'Transportation and Warehousing',\n",
        "                    51: 'Information',\n",
        "                    52: 'Finance and Insurance',\n",
        "                    53: 'Real Estate and Rental and Leasing',\n",
        "                    54: 'Professional, Scientific, and Technical Services',\n",
        "                    55: 'Management of Companies and Enterprises',\n",
        "                    56: 'Administrative and Support and Waste Management and Remediation Services',\n",
        "                    61: 'Educational Services',\n",
        "                    62: 'Health Care and Social Assistance',\n",
        "                    71: 'Arts, Entertainment, and Recreation',\n",
        "                    72: 'Accommodation and Food Services',\n",
        "                    81: 'Other Services (except Public Administration)',\n",
        "                    92: 'Public Administration'}"
      ],
      "metadata": {
        "id": "lCCEqWHaL-Nu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Economic_Sectoråˆ—ã‚’ä½œæˆã—ã€Numpyé…åˆ—å½¢å¼ã«å¤‰æ›ã—ã¦ã€å¾Œç¶šã®å‡¦ç†ã‚„ãƒ¢ãƒ‡ãƒ«å­¦ç¿’ã§ã®æ“ä½œæ€§ã‚’å‘ä¸Šã•ã›ã‚‹ã€‚\n",
        "\n",
        "\n",
        "# PrimaryNAICSåˆ—ã‹ã‚‰æœ€åˆã®2æ¡ã‚’æŠ½å‡ºã—ã¦æ•´æ•°å‹ã«å¤‰æ›\n",
        "train_df['first_two_digit_primary_naics'] = train_df['PrimaryNAICS'].apply(lambda z: str(z)[:2]).astype(int)\n",
        "test_df['first_two_digit_primary_naics']  = test_df['PrimaryNAICS'].apply(lambda z: str(z)[:2]).astype(int)\n",
        "\n",
        "# two_digit_mapã‚’ä½¿ç”¨ã—ã¦çµŒæ¸ˆã‚»ã‚¯ã‚¿ãƒ¼ã‚’å‰²ã‚Šå½“ã¦\n",
        "train_df['Economic_Sector']               = train_df['first_two_digit_primary_naics'].map(two_digit_map)\n",
        "test_df['Economic_Sector']                = test_df['first_two_digit_primary_naics'].map(two_digit_map)\n",
        "\n",
        "del train_df['first_two_digit_primary_naics'], test_df['first_two_digit_primary_naics'] #ä¸€æ™‚çš„ã«ä½¿ç”¨ã—ãŸåˆ—ã‚’å‰Šé™¤\n",
        "\n",
        "# Economic_Sectoråˆ—ã®å€¤ã‚’é…åˆ—ã«å¤‰æ›\n",
        "econ_sector_train                         = train_df['Economic_Sector'].values\n",
        "econ_sector_test                          = test_df['Economic_Sector'].values"
      ],
      "metadata": {
        "id": "dKTENguPMK_3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# çµŒæ¸ˆã‚»ã‚¯ã‚¿ãƒ¼ã«é–¢ã—ã¦æœ€ã‚‚è¿‘ã„ 5 ã¤ã®éš£æ¥è·é›¢ã‚’è¨ˆç®—ã—ã¾ã™\n",
        "\n",
        "def haversine(lat1, lon1, lat2, lon2):\n",
        "    # ç·¯åº¦ã¨çµŒåº¦ã‚’åº¦ã‹ã‚‰ãƒ©ã‚¸ã‚¢ãƒ³ã«å¤‰æ›ã™ã‚‹\n",
        "    lat1, lon1, lat2, lon2 = map(math.radians, [lat1, lon1, lat2, lon2])\n",
        "\n",
        "    # ãƒãƒ¼ãƒãƒ¼ã‚µã‚¤ãƒ³å¼ï¼ˆåœ°çƒä¸Šã®2ç‚¹é–“ã®è·é›¢ã‚’æ±‚ã‚ã‚‹ï¼‰\n",
        "    dlat = lat2 - lat1\n",
        "    dlon = lon2 - lon1\n",
        "    a = math.sin(dlat / 2)**2 + math.cos(lat1) * math.cos(lat2) * math.sin(dlon / 2)**2\n",
        "    c = 2 * math.atan2(math.sqrt(a), math.sqrt(1 - a))\n",
        "\n",
        "    # åœ°çƒã®åŠå¾„ï¼ˆã‚­ãƒ­ãƒ¡ãƒ¼ãƒˆãƒ«ï¼‰ï¼ˆå¹³å‡åŠå¾„ï¼‰\n",
        "    R = 6371.0\n",
        "\n",
        "    # è·é›¢ã‚’è¨ˆç®—ã™ã‚‹\n",
        "    distance = R * c\n",
        "    return distance"
      ],
      "metadata": {
        "id": "WacyQJCTMN8r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# å‘¨è¾ºè¦³æ¸¬åœ°ç‚¹ã®æƒ…å ±ã‚’è€ƒæ…®ã—ãŸç‰¹é‡é‡ã‚’ä½œæˆ\n",
        "\n",
        "# neighboursã®è¨­å®š\n",
        "neighbours = 5\n",
        "\n",
        "# ä¿å­˜ã™ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã®è¨­å®š\n",
        "train_distance_file = f\"/content/drive/MyDrive/data/train_distance_{neighbours}nbrs.csv\"\n",
        "test_distance_file = f\"/content/drive/MyDrive/data/test_distance_{neighbours}nbrs.csv\"\n",
        "\n",
        "# test_distanceã®èª­ã¿è¾¼ã¿ã¾ãŸã¯è¨ˆç®—\n",
        "if os.path.exists(test_distance_file):\n",
        "    print(f\"{test_distance_file} exists. Loading from file.\")\n",
        "    test_distance = pd.read_csv(test_distance_file)\n",
        "else:\n",
        "    output = []\n",
        "    for index in tqdm(range(test_df.shape[0])):\n",
        "        lat1 = test_df.iloc[index]['Latitude']\n",
        "        lon1 = test_df.iloc[index]['Longitude']\n",
        "        econ_sector_ref = test_df.iloc[index]['Economic_Sector']\n",
        "        x = get_nearest_distance(lat1, lon1, econ_sector_ref, neighbours=neighbours, train_point=False)\n",
        "        output.append(x)\n",
        "\n",
        "    test_distance = pd.DataFrame(output, columns=['Economy_Sector_Weighted_Avg', 'Economic_Sector_Average',\n",
        "                                                  'Nearest_Weighted_Average', 'Nearest_Average'])\n",
        "    test_distance.to_csv(test_distance_file, index=False)\n",
        "\n",
        "# train_distanceã®èª­ã¿è¾¼ã¿ã¾ãŸã¯è¨ˆç®—\n",
        "if os.path.exists(train_distance_file):\n",
        "    print(f\"{train_distance_file} exists. Loading from file.\")\n",
        "    train_distance = pd.read_csv(train_distance_file)\n",
        "else:\n",
        "    output = []\n",
        "    for index in tqdm(range(train_df.shape[0])):\n",
        "        lat1 = train_df.iloc[index]['Latitude']\n",
        "        lon1 = train_df.iloc[index]['Longitude']\n",
        "        econ_sector_ref = train_df.iloc[index]['Economic_Sector']\n",
        "        x = get_nearest_distance(lat1, lon1, econ_sector_ref, neighbours=neighbours, train_point=True)\n",
        "        output.append(x)\n",
        "\n",
        "    train_distance = pd.DataFrame(output, columns=['Economy_Sector_Weighted_Avg', 'Economic_Sector_Average',\n",
        "                                                   'Nearest_Weighted_Average', 'Nearest_Average'])\n",
        "    train_distance.to_csv(train_distance_file, index=False)\n",
        "\n",
        "# ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’train_dfã¨test_dfã«çµåˆ\n",
        "train_df = pd.concat((train_df, train_distance), axis=1)\n",
        "test_df  = pd.concat((test_df, test_distance), axis=1)\n"
      ],
      "metadata": {
        "id": "ObJ4ZnU_fH5l",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_distance(lat1, lon1, lat2, lon2):\n",
        "    return haversine(lat1, lon1, lat2, lon2)\n",
        "\n",
        "def get_nearest_distance(lat1, lon1, econ_sector_ref, neighbours=5, train_point=False):\n",
        "    # å¿…è¦ãªåˆ—ã‚’NumPyé…åˆ—ã«ä¸€åº¦æŠ½å‡ºã™ã‚‹\n",
        "    latitudes = train_df['Latitude'].values\n",
        "    longitudes = train_df['Longitude'].values\n",
        "    ghg_emissions = train_df['GHG_Direct_Emissions_14_in_metric_tons'].values\n",
        "    econ_sectors = train_df['Economic_Sector'].values\n",
        "\n",
        "    # NaNæ’å‡ºã‚’ç›´æ¥ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã™ã‚‹\n",
        "    valid_indices = ~np.isnan(ghg_emissions)\n",
        "\n",
        "    latitudes = latitudes[valid_indices]\n",
        "    longitudes = longitudes[valid_indices]\n",
        "    ghg_emissions = ghg_emissions[valid_indices]\n",
        "    econ_sectors = econ_sectors[valid_indices]\n",
        "\n",
        "    # ThreadPoolExecutorã‚’ä½¿ç”¨ã—ã¦è·é›¢è¨ˆç®—ã‚’ä¸¦åˆ—åŒ–ã™ã‚‹\n",
        "    with ThreadPoolExecutor() as executor:\n",
        "        distances = list(executor.map(calculate_distance,\n",
        "                                     [lat1]*len(latitudes),\n",
        "                                     [lon1]*len(longitudes),\n",
        "                                     latitudes,\n",
        "                                     longitudes))\n",
        "\n",
        "    # çµæœã‚’DataFrameã«çµåˆã™ã‚‹\n",
        "    near_df = pd.DataFrame({\n",
        "        'Distance': distances,\n",
        "        'GHG_emission_14': ghg_emissions,\n",
        "        'Economic_Sector': econ_sectors\n",
        "    })\n",
        "\n",
        "\n",
        "    if train_point:\n",
        "        near_df.sort_values(by='Distance', inplace=True)\n",
        "        near_df = near_df.dropna()\n",
        "        near_df = near_df.iloc[1:].reset_index(drop=True)\n",
        "    else:\n",
        "        near_df.sort_values(by='Distance', inplace=True)\n",
        "        near_df = near_df.dropna()\n",
        "\n",
        "    # çµŒæ¸ˆåˆ†é‡åˆ¥ã«çµã‚Šè¾¼ã‚€\n",
        "    nearest_locations_econ_sector = near_df[near_df['Economic_Sector'] == econ_sector_ref]\n",
        "\n",
        "    # çµŒæ¸ˆåˆ†é‡ã¨å…¨ä½“ã®ä¸¡æ–¹ã§æœ€ã‚‚è¿‘ã„ä¸Šä½Nã‚’å–å¾—ã—ã¾ã™\n",
        "    sub_near_econ = nearest_locations_econ_sector.head(neighbours)\n",
        "    sub_nearest_locations = near_df.head(neighbours)\n",
        "\n",
        "    # åŠ é‡å¹³å‡ã‚’è¨ˆç®—ã™ã‚‹\n",
        "    econ_weighted_average = (sub_near_econ['GHG_emission_14'] / np.where(sub_near_econ['Distance'] == 0, 1, sub_near_econ['Distance']) ).sum()\n",
        "    near_weighted_average = (sub_nearest_locations['GHG_emission_14'] / np.where(sub_nearest_locations['Distance']==0,1,sub_nearest_locations['Distance'])).sum()\n",
        "\n",
        "    # é€šå¸¸ã®å¹³å‡å€¤ã‚’è¨ˆç®—ã™ã‚‹\n",
        "    econ_average = sub_near_econ['GHG_emission_14'].mean()\n",
        "    near_average = sub_nearest_locations['GHG_emission_14'].mean()\n",
        "\n",
        "    return [econ_weighted_average, econ_average, near_weighted_average, near_average]"
      ],
      "metadata": {
        "id": "JBg1PUTFMSTq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TRIæ’å‡ºé‡ã®ç‰¹å¾´é‡ä½œæˆ\n",
        "\n",
        "def create_features1(df):\n",
        "\n",
        "    # å‰å¹´æ¯”\n",
        "    df['TRI_Air_Emissions_YoY_Change_11'] = df['TRI_Air_Emissions_11_in_lbs'] - df['TRI_Air_Emissions_10_in_lbs']\n",
        "    df['TRI_Air_Emissions_YoY_Change_12'] = df['TRI_Air_Emissions_12_in_lbs'] - df['TRI_Air_Emissions_11_in_lbs']\n",
        "    df['TRI_Air_Emissions_YoY_Change_13'] = df['TRI_Air_Emissions_13_in_lbs'] - df['TRI_Air_Emissions_12_in_lbs']\n",
        "\n",
        "   # å‰å¹´æ¯”å¢—åŠ ç‡\n",
        "    df['TRI_Air_Emissions_Growth_Rate_11'] = np.where(\n",
        "        df['TRI_Air_Emissions_10_in_lbs'].notna() & (df['TRI_Air_Emissions_10_in_lbs'] != 0),\n",
        "        (df['TRI_Air_Emissions_11_in_lbs'] - df['TRI_Air_Emissions_10_in_lbs']) / df['TRI_Air_Emissions_10_in_lbs'],\n",
        "        np.nan\n",
        "    )\n",
        "\n",
        "    df['TRI_Air_Emissions_Growth_Rate_12'] = np.where(\n",
        "        df['TRI_Air_Emissions_11_in_lbs'].notna() & (df['TRI_Air_Emissions_11_in_lbs'] != 0),\n",
        "        (df['TRI_Air_Emissions_12_in_lbs'] - df['TRI_Air_Emissions_11_in_lbs']) / df['TRI_Air_Emissions_11_in_lbs'],\n",
        "        np.nan\n",
        "    )\n",
        "\n",
        "    df['TRI_Air_Emissions_Growth_Rate_13'] = np.where(\n",
        "        df['TRI_Air_Emissions_12_in_lbs'].notna() & (df['TRI_Air_Emissions_12_in_lbs'] != 0),\n",
        "        (df['TRI_Air_Emissions_13_in_lbs'] - df['TRI_Air_Emissions_12_in_lbs']) / df['TRI_Air_Emissions_12_in_lbs'],\n",
        "        np.nan\n",
        "    )\n",
        "\n",
        "    return df\n",
        "\n",
        "train_df      = create_features1(train_df)\n",
        "test_df       = create_features1(test_df)\n",
        "new_features1 = ['TRI_Air_Emissions_YoY_Change_11','TRI_Air_Emissions_YoY_Change_12','TRI_Air_Emissions_YoY_Change_13',\n",
        "                 'TRI_Air_Emissions_Growth_Rate_11','TRI_Air_Emissions_Growth_Rate_12','TRI_Air_Emissions_Growth_Rate_13']"
      ],
      "metadata": {
        "id": "p_Udt9Z6kwrl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# GHGæ’å‡ºé‡ã®ç‰¹å¾´é‡ä½œæˆ\n",
        "\n",
        "def create_features2(df):\n",
        "\n",
        "    # å‰å¹´æ¯”\n",
        "    df['GHG_Direct_Emissions_YoY_Change_11'] = df['GHG_Direct_Emissions_11_in_metric_tons'] - df['GHG_Direct_Emissions_10_in_metric_tons']\n",
        "    df['GHG_Direct_Emissions_YoY_Change_12'] = df['GHG_Direct_Emissions_12_in_metric_tons'] - df['GHG_Direct_Emissions_11_in_metric_tons']\n",
        "    df['GHG_Direct_Emissions_YoY_Change_13'] = df['GHG_Direct_Emissions_13_in_metric_tons'] - df['GHG_Direct_Emissions_12_in_metric_tons']\n",
        "\n",
        "    # å‰å¹´æ¯”å¢—åŠ ç‡\n",
        "    df['GHG_Direct_Emissions_Growth_Rate_11'] = np.where(\n",
        "        df['GHG_Direct_Emissions_10_in_metric_tons'].notna() & (df['GHG_Direct_Emissions_10_in_metric_tons'] != 0),\n",
        "        (df['GHG_Direct_Emissions_11_in_metric_tons'] - df['GHG_Direct_Emissions_10_in_metric_tons']) / df['GHG_Direct_Emissions_10_in_metric_tons'],\n",
        "        np.nan\n",
        "    )\n",
        "\n",
        "    df['GHG_Direct_Emissions_Growth_Rate_12'] = np.where(\n",
        "        df['GHG_Direct_Emissions_11_in_metric_tons'].notna() & (df['GHG_Direct_Emissions_11_in_metric_tons'] != 0),\n",
        "        (df['GHG_Direct_Emissions_12_in_metric_tons'] - df['GHG_Direct_Emissions_11_in_metric_tons']) / df['GHG_Direct_Emissions_11_in_metric_tons'],\n",
        "        np.nan\n",
        "    )\n",
        "\n",
        "    df['GHG_Direct_Emissions_Growth_Rate_13'] = np.where(\n",
        "        df['GHG_Direct_Emissions_12_in_metric_tons'].notna() & (df['GHG_Direct_Emissions_12_in_metric_tons'] != 0),\n",
        "        (df['GHG_Direct_Emissions_13_in_metric_tons'] - df['GHG_Direct_Emissions_12_in_metric_tons']) / df['GHG_Direct_Emissions_12_in_metric_tons'],\n",
        "        np.nan\n",
        "    )\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "new_features2 = ['GHG_Direct_Emissions_YoY_Change_11','GHG_Direct_Emissions_YoY_Change_12','GHG_Direct_Emissions_YoY_Change_13',\n",
        "                 'GHG_Direct_Emissions_Growth_Rate_11','GHG_Direct_Emissions_Growth_Rate_12','GHG_Direct_Emissions_Growth_Rate_13'\n",
        "                 ]\n",
        "train_df      = create_features2(train_df)\n",
        "test_df       = create_features2(test_df)"
      ],
      "metadata": {
        "id": "sf0X49TO83PO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TRIã¨GHGã®æ¯”ç‡ã®ç‰¹å¾´é‡ä½œæˆ\n",
        "\n",
        "def create_features3(df):\n",
        "\n",
        "    # æ¯”ç‡è¨ˆç®—æ™‚ã«0é™¤ç®—ã‚’é˜²ããŸã‚ã®å°ã•ãªå€¤ã‚’è¿½åŠ \n",
        "    epsilon = 1e-6\n",
        "\n",
        "    # å„å¹´ã®TRIæ’å‡ºé‡ã¨GHGæ’å‡ºé‡ã®æ¯”ç‡ã‚’è¨ˆç®—\n",
        "    df['TRI_to_GHG_Ratio_10'] = df['TRI_Air_Emissions_10_in_lbs'] / (df['GHG_Direct_Emissions_10_in_metric_tons'] + epsilon)\n",
        "    df['TRI_to_GHG_Ratio_11'] = df['TRI_Air_Emissions_11_in_lbs'] / (df['GHG_Direct_Emissions_11_in_metric_tons'] + epsilon)\n",
        "    df['TRI_to_GHG_Ratio_12'] = df['TRI_Air_Emissions_12_in_lbs'] / (df['GHG_Direct_Emissions_12_in_metric_tons'] + epsilon)\n",
        "    df['TRI_to_GHG_Ratio_13'] = df['TRI_Air_Emissions_13_in_lbs'] / (df['GHG_Direct_Emissions_13_in_metric_tons'] + epsilon)\n",
        "\n",
        "    return df\n",
        "\n",
        "train_df      = create_features3(train_df)\n",
        "test_df       = create_features3(test_df)\n",
        "new_features3 = ['TRI_to_GHG_Ratio_10','TRI_to_GHG_Ratio_11','TRI_to_GHG_Ratio_12','TRI_to_GHG_Ratio_13']"
      ],
      "metadata": {
        "id": "o6Kg94US86tZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ğŸª„ ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†"
      ],
      "metadata": {
        "id": "ChntQDciuTI-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# çµŒæ¸ˆã‚»ã‚¯ã‚¿ãƒ¼ã¨å·ã«å¯¾ã—ã¦è¡Œã‚ã‚ŒãŸç›®æ¨™ã®é›†è¨ˆ\n",
        "\n",
        "# summary_df ã®ä½œæˆ\n",
        "summary_df = train_df.groupby(['Economic_Sector', 'State']).agg({\n",
        "    'GHG_Direct_Emissions_14_in_metric_tons': ['mean', 'median', 'max', 'min', 'count']\n",
        "})\n",
        "\n",
        "# åˆ—åã‚’ãƒªãƒãƒ¼ãƒ \n",
        "summary_df.columns = [\n",
        "    'GHG_Direct_Emissions_14_in_metric_tons_mean',\n",
        "    'GHG_Direct_Emissions_14_in_metric_tons_median',\n",
        "    'GHG_Direct_Emissions_14_in_metric_tons_max',\n",
        "    'GHG_Direct_Emissions_14_in_metric_tons_min',\n",
        "    'GHG_Direct_Emissions_14_in_metric_tons_count'\n",
        "]\n",
        "\n",
        "# ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ãƒªã‚»ãƒƒãƒˆã—ã¦ã€ã¾ã¨ã‚ãŸãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã«ã™ã‚‹\n",
        "summary_df = summary_df.reset_index()\n",
        "\n",
        "# train_df ã¨ test_df ã«å¯¾ã—ã¦ã€ summary_df ã‚’ left join\n",
        "train_df = train_df.merge(summary_df, on=['Economic_Sector', 'State'], how='left')\n",
        "test_df = test_df.merge(summary_df, on=['Economic_Sector', 'State'], how='left')\n"
      ],
      "metadata": {
        "id": "8gLIUUIOjRyD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ã‚«ãƒ†ã‚´ãƒªç‰¹å¾´é‡ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰\n",
        "\n",
        "merged_df = pd.concat((train_df,test_df),axis=0)\n",
        "for cols in ['City','State','County','FIPScode','PrimaryNAICS','Economic_Sector']:\n",
        "    le              = LabelEncoder()\n",
        "    merged_df[cols] = le.fit_transform(merged_df[cols].values.reshape(-1,1))\n",
        "\n",
        "train_df = merged_df.iloc[:train_df.shape[0],:]\n",
        "test_df  = merged_df.iloc[train_df.shape[0]:,:]\n",
        "train_df.shape,test_df.shape"
      ],
      "metadata": {
        "id": "XKnn5sTTMv-_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ğŸ‹ï¸ ãƒ¢ãƒ‡ãƒ«ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°"
      ],
      "metadata": {
        "id": "v-1Pobi-qkUa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ãƒ¢ãƒ‡ãƒ«ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ç”¨ã®æœ€çµ‚ãƒ‡ãƒ¼ã‚¿ã®ä½œæˆ\n",
        "\n",
        "numerical_columns = [\n",
        "                    'TRI_Air_Emissions_10_in_lbs', 'TRI_Air_Emissions_11_in_lbs',\n",
        "                    'TRI_Air_Emissions_12_in_lbs', 'TRI_Air_Emissions_13_in_lbs',\n",
        "                    'GHG_Direct_Emissions_10_in_metric_tons', 'GHG_Direct_Emissions_11_in_metric_tons',\n",
        "                    'GHG_Direct_Emissions_12_in_metric_tons', 'GHG_Direct_Emissions_13_in_metric_tons',\n",
        "                    ]\n",
        "\n",
        "categorical_columns = ['City','State','County','FIPScode','PrimaryNAICS','Economic_Sector']\n",
        "\n",
        "lat_lon_columns = ['Latitude','Longitude']\n",
        "\n",
        "new_features1  = [\n",
        "                  'TRI_Air_Emissions_YoY_Change_11','TRI_Air_Emissions_YoY_Change_12','TRI_Air_Emissions_YoY_Change_13',\n",
        "                  'TRI_Air_Emissions_Growth_Rate_11','TRI_Air_Emissions_Growth_Rate_12','TRI_Air_Emissions_Growth_Rate_13'\n",
        "                 ]\n",
        "\n",
        "new_features2  = [\n",
        "                  'GHG_Direct_Emissions_YoY_Change_11','GHG_Direct_Emissions_YoY_Change_12','GHG_Direct_Emissions_YoY_Change_13',\n",
        "                  'GHG_Direct_Emissions_Growth_Rate_11','GHG_Direct_Emissions_Growth_Rate_12','GHG_Direct_Emissions_Growth_Rate_13'\n",
        "                 ]\n",
        "\n",
        "new_features3 = ['TRI_to_GHG_Ratio_10','TRI_to_GHG_Ratio_11','TRI_to_GHG_Ratio_12','TRI_to_GHG_Ratio_13']\n",
        "\n",
        "target_columns = ['GHG_Direct_Emissions_14_in_metric_tons']\n",
        "\n",
        "train = train_df[numerical_columns+\n",
        "                 lat_lon_columns+\n",
        "                # categorical_columns+\n",
        "                 new_features1+['PrimaryNAICS']\n",
        "                #  new_features2+\n",
        "                #  new_features3+\n",
        "                #  train_aggregations+\n",
        "                #  neighbour_feats\n",
        "                 ].values\n",
        "test  = test_df[numerical_columns+\n",
        "                 lat_lon_columns+\n",
        "                # categorical_columns+\n",
        "                 new_features1+['PrimaryNAICS']\n",
        "                #  new_features2+\n",
        "                #  new_features3+\n",
        "                #  train_aggregations+\n",
        "                #  neighbour_feats\n",
        "                 ].values\n",
        "target = train_df[target_columns].values"
      ],
      "metadata": {
        "id": "TIuR260RM2tr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ğŸ“ ãƒ¢ãƒ‡ãƒ«è©•ä¾¡"
      ],
      "metadata": {
        "id": "1xEw26bPvRrF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.ensemble import HistGradientBoostingRegressor\n",
        "from catboost import CatBoostRegressor\n",
        "\n",
        "# å„foldã§ã®äºˆæ¸¬çµæœã‚’æ ¼ç´ã™ã‚‹é…åˆ—ã®åˆæœŸåŒ–\n",
        "stacked_train = np.zeros((train.shape[0], 3))  # model_dictã«3ã¤ã®ãƒ¢ãƒ‡ãƒ«ãŒã‚ã‚‹å ´åˆ\n",
        "stacked_test = np.zeros((test.shape[0], 3))\n",
        "\n",
        "# ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ã®é‡ã¿\n",
        "weights = [0.2, 0.5, 0.3]  # Random Forest : CatBoost : LightGBM\n",
        "\n",
        "def get_models_trained(train, test, target, num_folds):\n",
        "    kf = KFold(n_splits=num_folds, shuffle=True, random_state=13)\n",
        "\n",
        "    oof_predictions = np.zeros(len(train))\n",
        "    test_predictions = np.zeros(len(test))\n",
        "\n",
        "    for fold, (train_index, valid_index) in enumerate(kf.split(train, target)):\n",
        "        X_train, X_valid = train[train_index], train[valid_index]\n",
        "        y_train, y_valid = target[train_index], target[valid_index]\n",
        "\n",
        "        # ãƒ¢ãƒ‡ãƒ«ã‚’å®šç¾©\n",
        "        # Random Forestãƒ¢ãƒ‡ãƒ«\n",
        "        rf_params = {\n",
        "            \"n_estimators\": 100,\n",
        "            \"max_depth\": 10,\n",
        "            \"random_state\": 13\n",
        "        }\n",
        "        model1 = RandomForestRegressor(**rf_params)\n",
        "\n",
        "        # CatBoostãƒ¢ãƒ‡ãƒ«\n",
        "        catboost_params = {\n",
        "            \"iterations\": 300,\n",
        "            \"learning_rate\": 0.03,\n",
        "            \"depth\": 6,\n",
        "            \"random_seed\": 13,\n",
        "            \"silent\": True\n",
        "        }\n",
        "        model2 = CatBoostRegressor(**catboost_params)\n",
        "\n",
        "        # LightGBMãƒ¢ãƒ‡ãƒ«\n",
        "        lgbm_params = {\n",
        "            \"n_estimators\": 100,\n",
        "            \"learning_rate\": 0.05,\n",
        "            \"max_depth\": 6,\n",
        "            \"random_state\": 13\n",
        "        }\n",
        "        model3 = LGBMRegressor(**lgbm_params)\n",
        "\n",
        "        # ãƒ¢ãƒ‡ãƒ«ãƒªã‚¹ãƒˆã«è¿½åŠ \n",
        "        models = [model1, model2, model3]  # Random Forest, CatBoost, LightGBM\n",
        "\n",
        "        for i, model in enumerate(models):\n",
        "            # ãƒ¢ãƒ‡ãƒ«ã®è¨“ç·´ã¨äºˆæ¸¬\n",
        "            _ = model.fit(X_train, np.log1p(y_train))\n",
        "            valid_preds = np.expm1(model.predict(X_valid))\n",
        "            test_preds = np.expm1(model.predict(test)) / kf.n_splits\n",
        "\n",
        "            # RMSLEã‚¹ã‚³ã‚¢ã®è¨ˆç®—\n",
        "            rmsle_score = root_mean_squared_log_error(y_valid, valid_preds)\n",
        "            print(f\"Fold {fold + 1} RMSLE for model {i + 1} = {rmsle_score}\")\n",
        "\n",
        "            # ã‚¹ã‚¿ãƒƒã‚­ãƒ³ã‚°ç”¨ã«äºˆæ¸¬çµæœã‚’æ ¼ç´\n",
        "            stacked_train[valid_index, i] = valid_preds\n",
        "            stacked_test[:, i] += test_preds\n",
        "\n",
        "    # å¹³å‡äºˆæ¸¬ã‚’é‡ã¿ä»˜ãã§ç®—å‡º\n",
        "    final_oof_predictions = np.sum(stacked_train * np.array(weights), axis=1)\n",
        "    final_test_predictions = np.sum(stacked_test * np.array(weights), axis=1)\n",
        "\n",
        "    # RMSLEã‚¹ã‚³ã‚¢ã®è¡¨ç¤º\n",
        "    oof_rmsle = root_mean_squared_log_error(target, final_oof_predictions)\n",
        "    print(f\"OOF RMSLE = {oof_rmsle}\")\n",
        "\n",
        "    return final_oof_predictions, final_test_predictions\n"
      ],
      "metadata": {
        "id": "qEawQCbC8xk9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# å®Ÿè¡Œ\n",
        "oof_predictions, test_predictions = get_models_trained(train, test, target, num_folds=30)"
      ],
      "metadata": {
        "id": "1rMm2dvSNcf7",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# æå‡ºãƒ•ã‚¡ã‚¤ãƒ«ã®ä½œæˆ\n",
        "submit = pd.read_csv('/content/drive/MyDrive/data/sample_submission.csv', header=None)\n",
        "submit [ 1 ]  =  test_predictions\n",
        "submit . to_csv ( 'submission_36_g.csv' ,  header = None ,  index = False )"
      ],
      "metadata": {
        "id": "lqS77bJCNE4j"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}